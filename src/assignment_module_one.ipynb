{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNBgGYg_lpVN"
      },
      "source": [
        "# **Product Recognition of Food Products**\n",
        "\n",
        "## Image Processing and Computer Vision - Assignment Module \\#1\n",
        "\n",
        "\n",
        "Contacts:\n",
        "\n",
        "- Prof. Giuseppe Lisanti -> giuseppe.lisanti@unibo.it\n",
        "- Prof. Samuele Salti -> samuele.salti@unibo.it\n",
        "- Alex Costanzino -> alex.costanzino@unibo.it\n",
        "- Francesco Ballerini -> francesco.ballerini4@unibo.it\n",
        "\n",
        "\n",
        "Computer vision-based object detection techniques can be applied in super market settings to build a system that can identify products on store shelves.\n",
        "An example of how this system could be used would be to assist visually impaired customers or automate common store management tasks like detecting low-stock or misplaced products, given an image of a shelf in a store."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW42NlZsyTv0"
      },
      "source": [
        "## Task\n",
        "Develop a computer vision system that, given a reference image for each product, is able to identify such product from one picture of a store shelf.\n",
        "\n",
        "<figure>\n",
        "<a href=\"https://imgbb.com/\">\n",
        "  <center>\n",
        "  <img src=\"https://i.ibb.co/TwkMWnH/Screenshot-2024-04-04-at-14-54-51.png\" alt=\"Screenshot-2024-04-04-at-14-54-51\" border=\"0\" width=\"300\" />\n",
        "</a>\n",
        "</figure>\n",
        "\n",
        "For each type of product displayed in the\n",
        "shelf the system should report:\n",
        "1. Number of instances;\n",
        "1. Dimension of each instance (width and height in pixel of the bounding box that enclose them);\n",
        "1. Position in the image reference system of each instance (center of the bounding box that enclose them).\n",
        "\n",
        "#### Example of expected output\n",
        "```\n",
        "Product 0 - 2 instance found:\n",
        "  Instance 1 {position: (256, 328), width: 57px, height: 80px}\n",
        "  Instance 2 {position: (311, 328), width: 57px, height: 80px}\n",
        "Product 1 â€“ 1 instance found:\n",
        ".\n",
        ".\n",
        ".\n",
        "```\n",
        "\n",
        "### Track A - Single Instance Detection\n",
        "Develop an object detection system to identify single instance of products given one reference image for each item and a scene image.\n",
        "\n",
        "The system should be able to correctly identify all the product in the shelves\n",
        "image.\n",
        "\n",
        "### Track B - Multiple Instances Detection\n",
        "In addition to what achieved at step A, the system should also be able to detect multiple instances of the same product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fIbZJKq16ba"
      },
      "source": [
        "## Data\n",
        "Two folders of images are provided:\n",
        "* **Models**: contains one reference image for each product that the system should be able to identify.\n",
        "* **Scenes**: contains different shelve pictures to test the developed algorithm in different scenarios. The images contained in this folder are corrupted by noise.\n",
        "\n",
        "#### Track A - Single Instance Detection\n",
        "* **Models**: {ref1.png to ref14.png}.\n",
        "* **Scenes**: {scene1.png to scene5.png}.\n",
        "\n",
        "#### Track B - Multiple Instances Detection\n",
        "* **Models**: {ref15.png to ref27.png}.\n",
        "* **Scenes**: {scene6.png to scene12.png}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjP3GCdujYlw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp -r /content/drive/MyDrive/AssignmentsIPCV/dataset.zip ./\n",
        "!unzip dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KRBeGbKsEDe"
      },
      "source": [
        "## Evaluation criteria\n",
        "1. **Procedural correctness**. There are several ways to solve the assignment. Design your own sound approach and justify every decision you make;\n",
        "\n",
        "2. **Clarity and conciseness**. Present your work in a readable way: format your code and comment every important step;\n",
        "\n",
        "3. **Correctness of results**. Try to solve as many instances as possible. You should be able to solve all the instances of the assignment, however, a thoroughly justified and sound procedure with a lower number of solved instances will be valued **more** than a poorly designed approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----\n",
        "----\n",
        "\n",
        "Project by:\n",
        "- Tian Cheng Xia\n",
        "- Luca Domeniconi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration variables\n",
        "\n",
        "DATASET_DIR = \"./dataset\"\n",
        "TEMPLATES_DIR_NAME = \"models\"\n",
        "SCENES_DIR_NAME = \"scenes\"\n",
        "TEMPLATES_FILE_NAME = \"ref{index}.png\"\n",
        "SCENE_FILE_NAME = \"scene{index}.png\"\n",
        "\n",
        "SINGLE_INSTANCE_TEMPLATE_IDXS = [ *range(1, 15) ]\n",
        "SINGLE_INSTANCE_SCENE_IDXS = [ *range(1, 6) ]\n",
        "MULTIPLE_INSTANCES_TEMPLATE_IDXS = [ *range(15, 27) ]\n",
        "MULTIPLE_INSTANCES_SCENE_IDXS = [ *range(6, 12) ]\n",
        "\n",
        "SHOW_PLOTS = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading images\n",
        "\n",
        "def loadImage(path):\n",
        "    image = cv2.imread(path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "template_dir = os.path.join(DATASET_DIR, TEMPLATES_DIR_NAME)\n",
        "scene_dir = os.path.join(DATASET_DIR, SCENES_DIR_NAME)\n",
        "\n",
        "single_instance_templates = [ loadImage( os.path.join(template_dir, TEMPLATES_FILE_NAME.format(index=i)) ) for i in SINGLE_INSTANCE_TEMPLATE_IDXS ]\n",
        "single_instance_scenes = [ loadImage( os.path.join(scene_dir, SCENE_FILE_NAME.format(index=i)) ) for i in SINGLE_INSTANCE_SCENE_IDXS ]\n",
        "multi_instance_templates = [ loadImage( os.path.join(template_dir, TEMPLATES_FILE_NAME.format(index=i)) ) for i in MULTIPLE_INSTANCES_TEMPLATE_IDXS ]\n",
        "multi_instance_scenes = [ loadImage( os.path.join(scene_dir, SCENE_FILE_NAME.format(index=i)) ) for i in MULTIPLE_INSTANCES_SCENE_IDXS ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We tested various combinations of filters following the rule-of-thumb of \n",
        "using a a median filter to remove salt-and-pepper noise followed by a filter to remove Gaussian noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter(image):\n",
        "    image = cv2.medianBlur(image, 9)\n",
        "    image = cv2.bilateralFilter(image, d=-1, sigmaColor=17, sigmaSpace=17)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single instance detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the single instance detection task, we employ the following workflow:\n",
        "\n",
        "**TODO**\n",
        "\n",
        "1. Find a candidate match in the scene.\n",
        "2. Compute the density coverage of the matched keypoints in the template image.\n",
        "3. Compute a confidence score based on the coverage. \n",
        "    If it is below a threshold, remove the matched area in the scene and return to point 1.\n",
        "    Otherwise, consider this match as the correct match.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Density coverage computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To solve the problem of instances that matches only a part of the template \n",
        "(e.g. same title on the cereal box but different lower part of the box), we decided to not only count\n",
        "the number of matches, but to subdivide the template in different parts, and then count how many of this parts have at least a match in the template.\n",
        "In this way, a partially matched template will give a lower match score even if the number\n",
        "of matches is high. In order to better subdivide the template we split it based on the density of\n",
        "the detected SIFT keypoints on the template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Splits the given rect in 4 rect if there are at least k points inside \n",
        "def split_rect(rect, points, k=5, max_depth=4):\n",
        "    # Rect is a tuple of 4 points\n",
        "    # rect = (upper_right, upper_left, lower_left, lower_right)\n",
        "    rects = []\n",
        "    \n",
        "    y_min, y_max = rect[3][1], rect[0][1]\n",
        "    x_min, x_max = rect[1][0], rect[0][0]\n",
        "    y_med, x_med = (y_max + y_min)/2, (x_max + x_min)/2\n",
        "    \n",
        "    inside_points = [p for p in points if p[0] >= x_min and p[0] <= x_max and p[1] >= y_min and p[1] <= y_max]\n",
        "    if len(inside_points) == 0:\n",
        "        return None\n",
        "    \n",
        "    if max_depth == 0:\n",
        "        return []\n",
        "    \n",
        "    if len(inside_points) > k:\n",
        "        upper_right_rect = ((x_max, y_max), (x_med, y_max), (x_med, y_med), (x_max, y_med))\n",
        "        upper_left_rect = ((x_med, y_max), (x_min, y_max), (x_min, y_med), (x_med, y_med))\n",
        "        lower_left_rect = ((x_med, y_med), (x_min, y_med), (x_min, y_min), (x_med, y_min))\n",
        "        lower_right_rect = ((x_max, y_med), (x_med, y_med), (x_med, y_min), ((x_max, y_min)))\n",
        "                \n",
        "        upper_right_split = split_rect(upper_right_rect, points, k, max_depth-1)\n",
        "        upper_left_split = split_rect(upper_left_rect, points, k, max_depth-1)\n",
        "        lower_left_split = split_rect(lower_left_rect, points, k, max_depth-1)\n",
        "        lower_right_split = split_rect(lower_right_rect, points, k, max_depth-1)\n",
        "\n",
        "        for s,r in [(upper_right_split, upper_right_rect), (upper_left_split, upper_left_rect), (lower_left_split, lower_left_rect), (lower_right_split, lower_right_rect)]:\n",
        "            if s is not None:\n",
        "                if len(s) > 0:\n",
        "                    rects += s\n",
        "                else:\n",
        "                    rects += [r]\n",
        "            \n",
        "    return rects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Alt text](https://i.ibb.co/fG7cwHj/output.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getDensity(matched_kp:list[tuple[float, float]], template_partition:list):\n",
        "    match_count = { s: 0 for s in template_partition }\n",
        "    \n",
        "    for x_kp, y_kp in matched_kp:\n",
        "        for section in template_partition:\n",
        "            (x_max, y_max), (_, _), (x_min, y_min), (_, _) = section\n",
        "            if (x_min <= x_kp <= x_max) and (y_min <= y_kp <= y_max):\n",
        "                match_count[section] += 1\n",
        "                break\n",
        "\n",
        "    # return [s for s in template_partition if match_count[s] > 0]\n",
        "    return sum([1 for s in template_partition if match_count[s] > 0]) / len(template_partition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Candidate match search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _polygonArea(points):\n",
        "    x = np.array([ pt[0] for pt in points ])\n",
        "    y = np.array([ pt[1] for pt in points ])\n",
        "    return 0.5 * np.abs( (x @ np.roll(y,1)) - (y @ np.roll(x,1)) )\n",
        "\n",
        "\n",
        "def _areVertexesOverflowing(vertexes, image, tolerance_ratio):\n",
        "    tolerance_x = image.shape[1] * tolerance_ratio\n",
        "    tolerance_y = image.shape[0] * tolerance_ratio\n",
        "\n",
        "    for point in vertexes:\n",
        "        if ((point[0] > image.shape[1]+tolerance_x) or (point[0] < -tolerance_x) or \n",
        "            (point[1] > image.shape[0]+tolerance_y) or (point[1] < -tolerance_y)): \n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def findInstance(\n",
        "        scene: np.ndarray, \n",
        "        template: np.ndarray, \n",
        "        to_try_thresholds: list[float] = np.arange(0.7, 0.9, 0.05),\n",
        "        overflow_tol_ratio: float = 0.15, \n",
        "        min_area_ratio: float = 0.01,\n",
        "        density_threshold: float = 0.40,\n",
        "        seed: int = 42\n",
        "    ):\n",
        "    \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "            scene : np.ndarray\n",
        "                Scene in which the object should be found.\n",
        "\n",
        "            template : np.ndarray\n",
        "                Template of the object to match.\n",
        "            \n",
        "            to_try_thresholds : list[float]\n",
        "                Threshold for the k-neighbors matching.\n",
        "\n",
        "            overflow_tol_ratio : float\n",
        "                TODO ...\n",
        "            \n",
        "            proj_min_area : float\n",
        "\n",
        "            seed : int\n",
        "                Random seed for non-deterministic functions.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "            object_vertexes : list[tuple[float, float]]\n",
        "                Vertexes of the boundaries of the object in the scene.\n",
        "    \"\"\"\n",
        "\n",
        "    sift = cv2.SIFT_create(\n",
        "        contrastThreshold = 0.015,\n",
        "        # edgeThreshold = 100000,\n",
        "    )\n",
        "\n",
        "    FLANN_INDEX_KDTREE = 1\n",
        "    flann = cv2.FlannBasedMatcher(\n",
        "        indexParams = { \"algorithm\": FLANN_INDEX_KDTREE, \"trees\": 5 },\n",
        "        searchParams = { \"checks\": 1000 }\n",
        "    )\n",
        "\n",
        "    curr_threshold_idx = 0\n",
        "    scene_area = scene.shape[0] * scene.shape[1]\n",
        "\n",
        "    while True:\n",
        "        # Find and match keypoints\n",
        "        kp_template = sift.detect(template)\n",
        "        kp_template, desc_template = sift.compute(template, kp_template)\n",
        "        kp_scene = sift.detect(scene)\n",
        "        kp_scene, desc_scene = sift.compute(scene, kp_scene)\n",
        "\n",
        "        cv2.setRNGSeed(seed)\n",
        "        matches = flann.knnMatch(desc_template, desc_scene, k=2)\n",
        "\n",
        "        good_matches = [ \n",
        "            m for m, n in matches \n",
        "            if m.distance < to_try_thresholds[curr_threshold_idx]*n.distance\n",
        "        ]\n",
        "\n",
        "        template_points = np.float32([ kp_template[m.queryIdx].pt for m in good_matches ]).reshape(-1, 2)\n",
        "        scene_points = np.float32([ kp_scene[m.trainIdx].pt for m in good_matches ]).reshape(-1, 2)\n",
        "        \n",
        "        # Find the homography from template to scene\n",
        "        try:\n",
        "            homography, mask = cv2.findHomography(template_points, scene_points, cv2.RANSAC, ransacReprojThreshold=20.0)\n",
        "            mask = mask[:, 0] == 1\n",
        "            if homography is None: raise Exception()\n",
        "        except:\n",
        "            curr_threshold_idx += 1\n",
        "            if curr_threshold_idx >= len(to_try_thresholds): break\n",
        "            continue\n",
        "            \n",
        "        # Compute the vertexes of the match object in the scene\n",
        "        h, w, _ = template.shape\n",
        "        object_vertexes = cv2.perspectiveTransform(\n",
        "            src = np.float32([ [0,0], [0,h-1], [w-1,h-1], [w-1,0] ]).reshape(-1, 1, 2), \n",
        "            m = homography\n",
        "        )\n",
        "        object_vertexes = [pt[0] for pt in object_vertexes]\n",
        "\n",
        "        splitted_template = split_rect(\n",
        "            rect = ((h, w), (0, w), (0, 0), (h, 0)),\n",
        "            points = [ (kp.pt[0], kp.pt[1]) for kp in kp_template ],\n",
        "            k = 5,\n",
        "            max_depth = 4\n",
        "        )\n",
        "\n",
        "        # Check if the projection is valid.\n",
        "        # It is not if:\n",
        "        # - it overflows too much from the borders of the image.\n",
        "        # - the area of the polygon is too small.\n",
        "        if ((_areVertexesOverflowing(object_vertexes, scene, overflow_tol_ratio)) or \n",
        "            (_polygonArea(object_vertexes) < scene_area*min_area_ratio) or\n",
        "            getDensity(template_points[mask, :], splitted_template) < density_threshold\n",
        "        ):\n",
        "            curr_threshold_idx += 1\n",
        "            if curr_threshold_idx >= len(to_try_thresholds): break\n",
        "        else:\n",
        "            return object_vertexes\n",
        "        \n",
        "        # Mask the found area\n",
        "        img_mask = Image.new('L', (scene.shape[1], scene.shape[0]), 0)\n",
        "        ImageDraw.Draw(img_mask).polygon([\n",
        "            (max(0, int(pt[0])), max(0, int(pt[1]))) for pt in object_vertexes\n",
        "        ], outline=1, fill=1)\n",
        "        mask = np.array(img_mask)\n",
        "        scene[ mask == 1, : ] = 0\n",
        "        \n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complete workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def singleInstanceDetection(scene, template):\n",
        "    scene = filter(scene)\n",
        "    object_vertexes = findInstance(scene, template)\n",
        "    return object_vertexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def formatResultBox(object_vertexes):\n",
        "    top_left = object_vertexes[0]\n",
        "    bottom_left = object_vertexes[1]\n",
        "    bottom_right = object_vertexes[2]\n",
        "    top_right = object_vertexes[3]\n",
        "\n",
        "    left_x_mean = int( (top_left[0] + bottom_left[0]) / 2 )\n",
        "    right_x_mean = int( (top_right[0] + bottom_right[0]) / 2 )\n",
        "    top_y_mean = int( (top_left[1] + top_right[1]) / 2 )\n",
        "    bottom_y_mean = int( (bottom_left[1] + bottom_right[1]) / 2 )\n",
        "\n",
        "    return (left_x_mean, top_y_mean), right_x_mean-left_x_mean, bottom_y_mean-top_y_mean\n",
        "\n",
        "for scene_idx, scene in zip(SINGLE_INSTANCE_SCENE_IDXS, single_instance_scenes):\n",
        "    for template_idx, template in zip(SINGLE_INSTANCE_TEMPLATE_IDXS, single_instance_templates):\n",
        "        object_vertexes = singleInstanceDetection(scene.copy(), template)\n",
        "        if object_vertexes is None: continue\n",
        "\n",
        "        # TODO: No match broken\n",
        "\n",
        "        top_left, width, height = formatResultBox(object_vertexes)\n",
        "\n",
        "        print(f\"Product {template_idx} - 1 instance found:\")\n",
        "        print(\"Instance 1 {{position: {position}, width: {width}px, height: {height}px}}\".format(position=top_left, width=width, height=height))\n",
        "\n",
        "        if SHOW_PLOTS:\n",
        "            plt.imshow(scene)\n",
        "            plt.gca().add_patch( patches.Rectangle(top_left, width, height, linewidth=1, fill=True, alpha=0.3, color=\"r\") )\n",
        "            plt.show()\n",
        "            print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple instances detection"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
